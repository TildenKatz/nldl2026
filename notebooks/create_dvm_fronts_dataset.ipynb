{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "e7640050",
   "metadata": {},
   "source": [
    "# DVM front-view\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f6acf451",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "from tqdm.auto import tqdm\n",
    "import torch\n",
    "import webdataset as wds\n",
    "import PIL.Image as Image\n",
    "import lightning as L\n",
    "import polars as pl\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "BASE = Path(\"~/Downloads/dvm\").expanduser()\n",
    "\n",
    "TABLES = BASE / \"tables_V2.0\"\n",
    "IMAGES = BASE / \"Confirmed_fronts\"\n",
    "OUTPUT = Path(\"data/dvm\")\n",
    "\n",
    "OUTPUT.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "SEED = 1337\n",
    "\n",
    "L.seed_everything(SEED)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "97b2b668",
   "metadata": {},
   "source": [
    "## Load tables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a6f90b2e",
   "metadata": {},
   "outputs": [],
   "source": [
    "ad_data = pl.read_csv(\n",
    "    TABLES / \"Ad_table (extra).csv\",\n",
    "    infer_schema_length=None,\n",
    ").with_columns(\n",
    "    # remove characters (some include unit)\n",
    "    pl.col(\"Runned_Miles\").str.replace_all(\"[^0-9]\", \"\").cast(pl.Float64),\n",
    "    pl.col(\"Engin_size\").str.replace_all(\"[^0-9]\", \"\").cast(pl.Float64),\n",
    ")\n",
    "\n",
    "ad_data.columns = [col.strip() for col in ad_data.columns]\n",
    "\n",
    "ad_data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a69a6026",
   "metadata": {},
   "outputs": [],
   "source": [
    "image_data = pl.read_csv(TABLES / \"Image_table.csv\")\n",
    "image_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "91e71dab",
   "metadata": {},
   "outputs": [],
   "source": [
    "price_data = pl.read_csv(TABLES / \"Price_table.csv\")\n",
    "price_data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "25ac46e6",
   "metadata": {},
   "source": [
    "## Filter to front-view"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b1103fb4",
   "metadata": {},
   "outputs": [],
   "source": [
    "front_images = (\n",
    "    image_data.filter(\n",
    "        (pl.col(\"Quality_check\") == \"P\"),\n",
    "        (pl.col(\"Predicted_viewpoint\") == 0),\n",
    "    )\n",
    "    .with_columns(\n",
    "        pl.col(\"Image_ID\")\n",
    "        .str.split(\"$$\")\n",
    "        .list.slice(0, 2)\n",
    "        .list.join(\"$$\")\n",
    "        .alias(\"Adv_ID\")\n",
    "    )\n",
    "    .unique(\"Adv_ID\")\n",
    ")\n",
    "\n",
    "front_images"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "41d1d1be",
   "metadata": {},
   "source": [
    "## Merge tables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c911d61e",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = ad_data.join(\n",
    "    price_data.select(\"Genmodel_ID\", \"Entry_price\", \"Year\"),\n",
    "    left_on=[\"Genmodel_ID\", \"Reg_year\"],\n",
    "    right_on=[\"Genmodel_ID\", \"Year\"],\n",
    ").join(\n",
    "    front_images.select(\"Adv_ID\", \"Image_name\"),\n",
    "    on=\"Adv_ID\",\n",
    ")\n",
    "\n",
    "assert df[\"Adv_ID\"].n_unique() == len(df)\n",
    "df\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0296d4b4",
   "metadata": {},
   "source": [
    "## Verify images exist"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0526c8b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def image_path(image_name: str) -> Path:\n",
    "    parts = image_name.split(\"$$\")\n",
    "    return IMAGES / parts[0] / parts[2] / image_name\n",
    "\n",
    "\n",
    "def image_exists(image_name: str) -> bool:\n",
    "    return image_path(image_name).is_file()\n",
    "\n",
    "\n",
    "# Check which images exist\n",
    "missing = {\n",
    "    image_name\n",
    "    for image_name in tqdm(df[\"Image_name\"], desc=\"Checking image paths\")\n",
    "    if not image_exists(image_name)\n",
    "}\n",
    "\n",
    "print(f\"Missing images: {len(missing):,} / {len(df):,}\")\n",
    "\n",
    "# Keep only samples with existing images\n",
    "df = df.filter(~pl.col(\"Image_name\").is_in(missing))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f3a821e0",
   "metadata": {},
   "source": [
    "## Process features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "21d63e17",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Drop rows with missing values in key columns\n",
    "df = df.drop_nulls(\n",
    "    (\n",
    "        \"Runned_Miles\",\n",
    "        \"Price\",\n",
    "        \"Engin_size\",\n",
    "        \"Seat_num\",\n",
    "        \"Door_num\",\n",
    "        \"Wheelbase\",\n",
    "        \"Height\",\n",
    "        \"Width\",\n",
    "        \"Length\",\n",
    "    )\n",
    ")\n",
    "print(f\"After dropping NaN: {len(df):,}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0c543fe5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "physical_features = (\"Wheelbase\", \"Height\", \"Width\", \"Length\")\n",
    "\n",
    "_n_rows = len(df)\n",
    "df = df.with_columns(\n",
    "    pl.col(col).add(np.random.randint(-50, 50, size=_n_rows))\n",
    "    for col in physical_features\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6a37db4b",
   "metadata": {},
   "source": [
    "## Remove infrequent car models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c384b348",
   "metadata": {},
   "outputs": [],
   "source": [
    "MIN_SAMPLES = 100\n",
    "\n",
    "# Count samples per model\n",
    "model_counts = df[\"Genmodel_ID\"].value_counts()\n",
    "valid_models = model_counts.filter(pl.col(\"count\") >= MIN_SAMPLES)[\n",
    "    \"Genmodel_ID\"\n",
    "].to_list()\n",
    "\n",
    "print(f\"Models with >= {MIN_SAMPLES} samples: {len(valid_models)}\")\n",
    "print(f\"Models removed: {len(model_counts) - len(valid_models)}\")\n",
    "\n",
    "\n",
    "df = df.filter(pl.col(\"Genmodel_ID\").is_in(valid_models))\n",
    "print(f\"Final dataset size: {len(df):,}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e9056ceb",
   "metadata": {},
   "outputs": [],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cb91dd8f",
   "metadata": {},
   "source": [
    "# Enumerate model labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "799596d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Readable model names\n",
    "df = df.with_columns(\n",
    "    pl.concat_str(\n",
    "        \"Maker\",\n",
    "        \"Genmodel\",\n",
    "        separator=\" \",\n",
    "    ).alias(\"model_name\"),\n",
    ").with_columns(\n",
    "    # change all 21_1 model names to \"Citroen DS3\" and all 7_11 to \"Audi A6\"\n",
    "    pl.when(pl.col(\"Genmodel_ID\").eq(\"21_1\"))\n",
    "    .then(pl.lit(\"Citroen DS3\"))\n",
    "    .when(pl.col(\"Genmodel_ID\").eq(\"7_11\"))\n",
    "    .then(pl.lit(\"Audi A6\"))\n",
    "    .otherwise(pl.col(\"model_name\"))\n",
    "    .alias(\"model_name\")\n",
    ")\n",
    "df[\"model_name\"].unique()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3f05e421",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remap Genmodel_ID to contiguous integers\n",
    "model_to_idx = (\n",
    "    df.select(\"Genmodel_ID\", \"model_name\")\n",
    "    .unique()\n",
    "    .sort(\"Genmodel_ID\")\n",
    "    .with_row_index()\n",
    "    .rename({\"index\": \"label\"})\n",
    ")\n",
    "model_to_idx\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8a567b26",
   "metadata": {},
   "outputs": [],
   "source": [
    "# assert no duplicates in any col of model_to_idx\n",
    "assert (\n",
    "    model_to_idx.group_by(\"Genmodel_ID\")\n",
    "    .agg(\"model_name\")\n",
    "    .filter(pl.col(\"model_name\").list.len() > 1)\n",
    "    .is_empty()\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0db8d2d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add label column to df\n",
    "df = df.join(\n",
    "    model_to_idx.select(\"Genmodel_ID\", \"label\"),\n",
    "    on=\"Genmodel_ID\",\n",
    ")\n",
    "\n",
    "n_classes = len(model_to_idx)\n",
    "print(f\"Number of classes: {n_classes}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f4102255",
   "metadata": {},
   "source": [
    "## Splits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5eef3b9e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Stratified split: 80% train+val, 20% test\n",
    "train_val_ids, test_ids = train_test_split(\n",
    "    df[\"Adv_ID\"].to_list(),\n",
    "    test_size=0.2,\n",
    "    random_state=SEED,\n",
    "    stratify=df[\"label\"].to_list(),\n",
    ")\n",
    "\n",
    "# Split train+val: 80% train, 20% val (of the 80%)\n",
    "train_val_df = df.filter(pl.col(\"Adv_ID\").is_in(train_val_ids))\n",
    "train_ids, val_ids = train_test_split(\n",
    "    train_val_df[\"Adv_ID\"].to_list(),\n",
    "    test_size=0.2,\n",
    "    random_state=SEED,\n",
    "    stratify=train_val_df[\"label\"].to_list(),\n",
    ")\n",
    "\n",
    "print(f\"Train: {len(train_ids):,}, Val: {len(val_ids):,}, Test: {len(test_ids):,}\")\n",
    "print(\n",
    "    f\"Ratios: {len(train_ids) / len(df):.1%} / {len(val_ids) / len(df):.1%} / {len(test_ids) / len(df):.1%}\"\n",
    ")\n",
    "\n",
    "train_df = df.filter(pl.col(\"Adv_ID\").is_in(train_ids)).sample(\n",
    "    fraction=1.0,\n",
    "    with_replacement=False,\n",
    "    seed=SEED,\n",
    "    shuffle=True,\n",
    ")\n",
    "\n",
    "val_df = df.filter(pl.col(\"Adv_ID\").is_in(val_ids))\n",
    "test_df = df.filter(pl.col(\"Adv_ID\").is_in(test_ids))\n",
    "\n",
    "splits = {\"train\": train_df, \"val\": val_df, \"test\": test_df}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6b6cd00f",
   "metadata": {},
   "source": [
    "## Tabular features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dc4c7cd2",
   "metadata": {},
   "outputs": [],
   "source": [
    "continuous_cols = (\n",
    "    \"Adv_year\",\n",
    "    \"Adv_month\",\n",
    "    \"Reg_year\",\n",
    "    \"Runned_Miles\",\n",
    "    \"Price\",\n",
    "    \"Seat_num\",\n",
    "    \"Door_num\",\n",
    "    \"Entry_price\",\n",
    "    \"Engin_size\",\n",
    "    *physical_features,\n",
    ")\n",
    "\n",
    "categorical_cols = (\n",
    "    \"Color\",\n",
    "    \"Bodytype\",\n",
    "    \"Gearbox\",\n",
    "    \"Fuel_type\",\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0d5b323e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Normalise continuous features\n",
    "train_means = train_df[continuous_cols].mean()\n",
    "train_stds = train_df[continuous_cols].std()\n",
    "\n",
    "splits = {\n",
    "    k: v.with_columns(\n",
    "        pl.col(col).sub(train_means[col]).truediv(train_stds[col]).alias(f\"{col}_norm\")\n",
    "        for col in continuous_cols\n",
    "    )\n",
    "    for k, v in splits.items()\n",
    "}\n",
    "\n",
    "print(\"Normalised continuous features\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4f0130a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Encode categorical features\n",
    "cat_mappings = {\n",
    "    col: {\n",
    "        value: index\n",
    "        for index, value in enumerate(train_df[col].drop_nulls().unique().sort())\n",
    "    }\n",
    "    for col in categorical_cols\n",
    "}\n",
    "\n",
    "# Save category sizes for embeddings\n",
    "print(\n",
    "    f\"Categorical sizes: { {col: len(mapping) for col, mapping in cat_mappings.items()} }\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f415d9c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Apply to all splits\n",
    "splits = {\n",
    "    k: v.with_columns(\n",
    "        pl.col(col)\n",
    "        .replace_strict(cat_mappings[col], default=-1, return_dtype=pl.Int32)\n",
    "        .alias(f\"{col}_enc\")\n",
    "        for col in categorical_cols\n",
    "    )\n",
    "    for k, v in splits.items()\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b0170bac",
   "metadata": {},
   "source": [
    "## Save webdataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ccd52b01",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Feature columns to save\n",
    "norm_cols = [f\"{c}_norm\" for c in continuous_cols]\n",
    "enc_cols = [f\"{c}_enc\" for c in categorical_cols]\n",
    "feature_cols = [*norm_cols, *enc_cols]\n",
    "\n",
    "# Shard configuration: ~100MB per shard, similar to wells dataset\n",
    "SHARD_SIZE = 1e8  # 100 MB\n",
    "\n",
    "for name, split_df in splits.items():\n",
    "    pattern = str(OUTPUT / f\"dvm_fronts_{name}-%04d.tar\")\n",
    "    \n",
    "    with wds.ShardWriter(pattern, maxsize=SHARD_SIZE) as sink:  # type: ignore\n",
    "        for row in tqdm(\n",
    "            split_df.iter_rows(named=True),\n",
    "            desc=f\"Saving {name} split to webdataset\",\n",
    "            total=len(split_df),\n",
    "        ):\n",
    "            sink.write(\n",
    "                {\n",
    "                    \"__key__\": str(row[\"Adv_ID\"]),\n",
    "                    \"label.pth\": torch.tensor(row[\"label\"]),\n",
    "                    \"features.pth\": torch.tensor(\n",
    "                        [row[col] for col in feature_cols], dtype=torch.float32\n",
    "                    ),\n",
    "                    \"image.jpg\": Path(image_path(row[\"Image_name\"])).read_bytes(),\n",
    "                }\n",
    "            )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4d34d434",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "# Count shards per split\n",
    "shard_counts = {}\n",
    "for name in splits:\n",
    "    shards = list(OUTPUT.glob(f\"dvm_fronts_{name}-*.tar\"))\n",
    "    shard_counts[name] = len(shards)\n",
    "    print(f\"{name}: {len(shards)} shards\")\n",
    "\n",
    "# Save metadata\n",
    "metadata = {\n",
    "    \"n_classes\": n_classes,\n",
    "    \"n_continuous\": len(continuous_cols),\n",
    "    \"n_categorical\": len(categorical_cols),\n",
    "    \"continuous_cols\": continuous_cols,\n",
    "    \"categorical_cols\": categorical_cols,\n",
    "    \"cat_sizes\": {col: len(cat_mappings[col]) for col in categorical_cols},\n",
    "    \"train_means\": {col: train_means[col].item() for col in continuous_cols},\n",
    "    \"train_stds\": {col: train_stds[col].item() for col in continuous_cols},\n",
    "    \"feature_cols\": feature_cols,\n",
    "    \"cat_mappings\": cat_mappings,\n",
    "    \"label_to_model\": {\n",
    "        row[\"label\"]: row[\"model_name\"] for row in model_to_idx.iter_rows(named=True)\n",
    "    },\n",
    "    \"label_to_id\": {\n",
    "        row[\"label\"]: row[\"Genmodel_ID\"] for row in model_to_idx.iter_rows(named=True)\n",
    "    },\n",
    "    \"split_sizes\": {name: len(split_df) for name, split_df in splits.items()},\n",
    "    \"shard_counts\": shard_counts,\n",
    "}\n",
    "(OUTPUT / \"metadata.json\").write_text(json.dumps(metadata, indent=4))\n",
    "\n",
    "print(\"Saved metadata\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8fa45f73",
   "metadata": {},
   "source": [
    "## Summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "040b6822",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"=\" * 50)\n",
    "print(\"DVM Front-View Dataset Created\")\n",
    "print(\"=\" * 50)\n",
    "print(f\"Classes: {n_classes}\")\n",
    "print(f\"Train: {len(train_df):,}\")\n",
    "print(f\"Val: {len(val_df):,}\")\n",
    "print(f\"Test: {len(test_df):,}\")\n",
    "print(f\"Total: {len(df):,}\")\n",
    "print(f\"\\nFeatures: {len(feature_cols)}\")\n",
    "print(f\"  Continuous: {continuous_cols}\")\n",
    "print(f\"  Categorical: {categorical_cols}\")\n",
    "print(f\"\\nOutput: {OUTPUT}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3844d7db",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "nldl-winter-school",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
