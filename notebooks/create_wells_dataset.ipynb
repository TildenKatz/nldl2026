{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "e7640050",
   "metadata": {},
   "source": [
    "# RWI: image, XRF and XRD"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f6acf451",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "from tqdm.auto import tqdm\n",
    "import torch\n",
    "import webdataset as wds\n",
    "\n",
    "import lasio\n",
    "import polars as pl\n",
    "\n",
    "import lightning as L\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "BASE = Path(\"~/Developer/cutting_cbir/data/cuttings\")\n",
    "TABLES = BASE / \"las\"\n",
    "IMAGES = BASE / \"raw\"\n",
    "OUTPUT = Path(\"data/rwi\")\n",
    "\n",
    "OUTPUT.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "SEED = 1337\n",
    "\n",
    "PATCH_SIZE = 256\n",
    "GRID_ROWS = 2\n",
    "GRID_COLS = 5\n",
    "\n",
    "L.seed_everything(SEED)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "97b2b668",
   "metadata": {},
   "source": [
    "## Load tables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "328ebecd",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_las(f: Path) -> pl.DataFrame:\n",
    "    las = lasio.read(f.as_posix())\n",
    "    well_name: str = las.well[\"WELL\"].value\n",
    "    quad, rest = well_name.split(\"/\", maxsplit=1)\n",
    "    well_name = f\"{quad}_{rest}\"\n",
    "    df = pl.from_pandas(las.df(), include_index=True).with_columns(\n",
    "        pl.lit(well_name).alias(\"well_name\")\n",
    "    )\n",
    "    return df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a6f90b2e",
   "metadata": {},
   "outputs": [],
   "source": [
    "xrf_data = pl.concat(\n",
    "    (load_las(f) for f in tqdm(TABLES.glob(\"*_XRF_1.LAS\"), desc=\"Loading LAS files\")),\n",
    "    how=\"diagonal_relaxed\",\n",
    ")\n",
    "xrf_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ee932a2e",
   "metadata": {},
   "outputs": [],
   "source": [
    "xrd_data = pl.concat(\n",
    "    (load_las(f) for f in tqdm(TABLES.glob(\"*_XRD_*.LAS\"), desc=\"Loading LAS files\")),\n",
    "    how=\"diagonal_relaxed\",\n",
    ")\n",
    "xrd_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a9200040",
   "metadata": {},
   "outputs": [],
   "source": [
    "def parse_depth(f: Path) -> float:\n",
    "    return round(float(f.stem.split(\"_\")[-3])) / 100\n",
    "\n",
    "\n",
    "def parse_well_name(f: Path) -> str:\n",
    "    return f.parent.name\n",
    "\n",
    "\n",
    "images_data = pl.DataFrame(\n",
    "    {\n",
    "        \"well_name\": parse_well_name(f),\n",
    "        \"DEPTH\": parse_depth(f),\n",
    "        \"image\": f.stem,\n",
    "    }\n",
    "    for f in tqdm(IMAGES.glob(\"**/*.CR2\"), desc=\"Loading image paths\")\n",
    ")\n",
    "images_data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "41d1d1be",
   "metadata": {},
   "source": [
    "## Merge tables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eeb32c83",
   "metadata": {},
   "outputs": [],
   "source": [
    "images_data[\"well_name\"].value_counts().sort(\"well_name\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "557c2836",
   "metadata": {},
   "outputs": [],
   "source": [
    "for w, count in xrf_data[\"well_name\"].value_counts().sort(\"well_name\").iter_rows():\n",
    "    print(f\"{w}: {count}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c911d61e",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = xrf_data.join(\n",
    "    xrd_data,\n",
    "    on=(\"DEPTH\", \"well_name\"),\n",
    ").join(\n",
    "    images_data,\n",
    "    on=(\n",
    "        \"DEPTH\",\n",
    "        \"well_name\",\n",
    "    ),\n",
    ")\n",
    "\n",
    "assert df[\"image\"].n_unique() == len(df)\n",
    "df\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f3a821e0",
   "metadata": {},
   "source": [
    "## Process features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "21d63e17",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.null_count().transpose(include_header=True).filter(pl.col(\"column_0\") > 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "da2b9f15",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df.drop(\"BALANCE\").with_columns(\n",
    "    pl.col(\"MGO\").fill_null(0.0),\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f4102255",
   "metadata": {},
   "source": [
    "## Splits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5eef3b9e",
   "metadata": {},
   "outputs": [],
   "source": [
    "wells = df[\"well_name\"].unique().sort().to_list()\n",
    "print(f\"Total wells: {len(wells):,}\")\n",
    "\n",
    "# Stratified split: 80% train+val, 20% test\n",
    "train_val_wells, test_wells = train_test_split(\n",
    "    wells,\n",
    "    test_size=0.2,\n",
    "    random_state=SEED,\n",
    ")\n",
    "\n",
    "# Split train+val: 80% train, 20% val (of the 80%)\n",
    "train_val_df = df.filter(pl.col(\"well_name\").is_in(train_val_wells))\n",
    "train_wells, val_wells = train_test_split(\n",
    "    train_val_wells,\n",
    "    test_size=0.2,\n",
    "    random_state=SEED,\n",
    ")\n",
    "\n",
    "print(\n",
    "    f\"Train: {len(train_wells):,}, Val: {len(val_wells):,}, Test: {len(test_wells):,}\"\n",
    ")\n",
    "\n",
    "train_df = df.filter(pl.col(\"well_name\").is_in(train_wells)).sample(\n",
    "    fraction=1.0,\n",
    "    with_replacement=False,\n",
    "    seed=SEED,\n",
    "    shuffle=True,\n",
    ")\n",
    "\n",
    "val_df = df.filter(pl.col(\"well_name\").is_in(val_wells))\n",
    "test_df = df.filter(pl.col(\"well_name\").is_in(test_wells))\n",
    "\n",
    "splits = {\"train\": train_df, \"val\": val_df, \"test\": test_df}\n",
    "\n",
    "print(\n",
    "    f\"Ratios: {len(train_df) / len(df):.1%} / {len(val_df) / len(df):.1%} / {len(test_df) / len(df):.1%}\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6b6cd00f",
   "metadata": {},
   "source": [
    "## Tabular features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dc4c7cd2",
   "metadata": {},
   "outputs": [],
   "source": [
    "index_cols = [\"well_name\", \"DEPTH\", \"image\"]\n",
    "xrd_cols = [col for col in df.columns if col.startswith(\"XRD_\")]\n",
    "xrf_cols = [col for col in df.columns if col not in index_cols and col not in xrd_cols]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0d5b323e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Normalise continuous features\n",
    "train_means = train_df[xrf_cols].mean()\n",
    "train_stds = train_df[xrf_cols].std()\n",
    "\n",
    "splits = {\n",
    "    k: v.with_columns(\n",
    "        pl.col(col).sub(train_means[col]).truediv(train_stds[col]).alias(f\"{col}_norm\")\n",
    "        for col in xrf_cols\n",
    "    )\n",
    "    for k, v in splits.items()\n",
    "}\n",
    "\n",
    "print(\"Normalised continuous features\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b0170bac",
   "metadata": {},
   "source": [
    "## Save webdataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "57808818",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "from typing import cast\n",
    "\n",
    "import numpy as np\n",
    "import numpy.typing as npt\n",
    "\n",
    "import rawpy\n",
    "\n",
    "\n",
    "def read_cr2_image(file_path: Path) -> npt.NDArray[np.uint8]:\n",
    "    # https://letmaik.github.io/rawpy/api/rawpy.Params.html\n",
    "    # https://www.libraw.org/docs/API-datastruct-eng.html\n",
    "    with rawpy.imread(file_path.absolute().as_posix()) as raw:\n",
    "        rgb_image = raw.postprocess(\n",
    "            # demosaicing / de-noise\n",
    "            demosaic_algorithm=rawpy.DemosaicAlgorithm.AHD,  # default\n",
    "            fbdd_noise_reduction=rawpy.FBDDNoiseReductionMode.Off,\n",
    "            median_filter_passes=0,\n",
    "            # white balance: fixed\n",
    "            use_camera_wb=False,\n",
    "            use_auto_wb=False,\n",
    "            user_wb=(2.0, 1.0, 1.3, 0.0),  # daylight balanced\n",
    "            # color space + bit depth\n",
    "            output_color=rawpy.ColorSpace.sRGB,\n",
    "            output_bps=8,\n",
    "            # exposure / brightness / scaling\n",
    "            no_auto_bright=True,  # disable LibRaw auto brightness\n",
    "            auto_bright_thr=0.00,  # ignored if no_auto_bright=True\n",
    "            bright=1.5,  # no arbitrary brightness scale\n",
    "            exp_shift=None,  # no exposure shift\n",
    "            exp_preserve_highlights=0.0,\n",
    "            no_auto_scale=False,  # KEEP scaling, you want proper WB + normalization\n",
    "            # black/white levels\n",
    "            user_black=512,  # all images have black level ~512\n",
    "            user_sat=0.0,  # no user saturation level\n",
    "            adjust_maximum_thr=0.0,  # no automatic white level adjustment\n",
    "            # gamma / tone curve\n",
    "            gamma=(2.222, 4.5),  # default, BT.709\n",
    "            # misc\n",
    "            highlight_mode=rawpy.HighlightMode.Clip,  # dont recover highlights\n",
    "            user_flip=0,  # no gyro shenanigans\n",
    "        )\n",
    "\n",
    "    rgb_image = cast(\n",
    "        npt.NDArray[np.uint8],\n",
    "        rgb_image,\n",
    "    )\n",
    "\n",
    "    clip_width = 12\n",
    "    clip_height = 10\n",
    "\n",
    "    rgb_image = rgb_image[clip_height:-clip_height, clip_width:-clip_width, :]\n",
    "\n",
    "    return rgb_image\n",
    "\n",
    "\n",
    "\n",
    "def extract_grid_patches(\n",
    "    well_name: str,\n",
    "    image_path: str,\n",
    ") -> npt.NDArray[np.uint8]:\n",
    "    path = IMAGES / well_name / f\"{image_path}.CR2\"\n",
    "    rgb_image = read_cr2_image(path)\n",
    "    img_height, img_width, _ = rgb_image.shape\n",
    "\n",
    "    row_stride = (img_height - PATCH_SIZE) // (GRID_ROWS - 1)\n",
    "    col_stride = (img_width - PATCH_SIZE) // (GRID_COLS - 1)\n",
    "\n",
    "    patches = []\n",
    "    for r in range(GRID_ROWS):\n",
    "        for c in range(GRID_COLS):\n",
    "            top = r * row_stride\n",
    "            left = c * col_stride\n",
    "            patch = rgb_image[top : top + PATCH_SIZE, left : left + PATCH_SIZE, :]\n",
    "            patches.append(patch)\n",
    "\n",
    "    return np.stack(patches)\n",
    "\n",
    "\n",
    "def extract_centre_patch(\n",
    "    well_name: str,\n",
    "    image_path: str,\n",
    ") -> npt.NDArray[np.uint8]:\n",
    "    path = IMAGES / well_name / f\"{image_path}.CR2\"\n",
    "    rgb_image = read_cr2_image(path)\n",
    "    patch = rgb_image[1872:2128, 2872:3128, :]\n",
    "    return patch[np.newaxis, ...]\n",
    "\n",
    "\n",
    "import vizy\n",
    "\n",
    "vizy.plot(list(extract_grid_patches(df[\"well_name\"][0], df[\"image\"][0])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ccd52b01",
   "metadata": {},
   "outputs": [],
   "source": [
    "feature_cols = [f\"{c}_norm\" for c in xrf_cols]\n",
    "\n",
    "\n",
    "def process_single_image(\n",
    "    row: dict[str, str | float],\n",
    "    split: str,\n",
    ") -> dict[str, object] | None:\n",
    "    try:\n",
    "        if split == \"train\":\n",
    "            patches = extract_grid_patches(row[\"well_name\"], row[\"image\"])\n",
    "        else:\n",
    "            patches = extract_centre_patch(row[\"well_name\"], row[\"image\"])\n",
    "\n",
    "        return {\n",
    "            \"__key__\": row[\"image\"],\n",
    "            \"label.pth\": torch.tensor(\n",
    "                [row[col] for col in xrd_cols], dtype=torch.float32\n",
    "            ),\n",
    "            \"features.pth\": torch.tensor(\n",
    "                [row[col] for col in feature_cols], dtype=torch.float32\n",
    "            ),\n",
    "            \"patches.pth\": torch.tensor(patches, dtype=torch.uint8),\n",
    "        }\n",
    "    except Exception as e:\n",
    "        print(f\"Error processing {row['image']}: {e}\")\n",
    "        return None\n",
    "\n",
    "\n",
    "from concurrent.futures import ThreadPoolExecutor\n",
    "from functools import partial\n",
    "\n",
    "OUTPUT.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "shard_counts: dict[str, int] = {}\n",
    "\n",
    "for name, split_df in splits.items():\n",
    "    pattern = str(OUTPUT / f\"{name}-%04d.tar\")\n",
    "    rows = list(split_df.iter_rows(named=True))\n",
    "    process_fn = partial(process_single_image, split=name)\n",
    "\n",
    "    shard_idx = 0\n",
    "    with wds.ShardWriter(  # type: ignore\n",
    "        pattern,\n",
    "        maxsize=2e8, # ~200MB per shard\n",
    "    ) as sink:\n",
    "        with ThreadPoolExecutor(max_workers=10) as executor:\n",
    "            for sample in tqdm(\n",
    "                executor.map(process_fn, rows),\n",
    "                total=len(rows),\n",
    "                desc=f\"Processing {name} split\",\n",
    "            ):\n",
    "                if sample is not None:\n",
    "                    sink.write(sample)\n",
    "        shard_idx = sink.shard\n",
    "\n",
    "    shard_counts[name] = shard_idx + 1\n",
    "    print(f\"{name}: {shard_counts[name]} shards\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4d34d434",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "metadata = {\n",
    "    \"xrd\": xrd_cols,\n",
    "    \"xrf\": xrf_cols,\n",
    "    \"xrf_means\": {col: train_means[col].item() for col in xrf_cols},\n",
    "    \"xrf_stds\": {col: train_stds[col].item() for col in xrf_cols},\n",
    "    \"split_sizes\": {\n",
    "        name: len(split_df) for name, split_df in splits.items()\n",
    "    },\n",
    "    \"split_wells\": {\n",
    "        name: split_df[\"well_name\"].unique().sort().to_list()\n",
    "        for name, split_df in splits.items()\n",
    "    },\n",
    "    \"shard_counts\": shard_counts,\n",
    "}\n",
    "(OUTPUT / \"metadata.json\").write_text(json.dumps(metadata, indent=4))\n",
    "\n",
    "print(\"Saved metadata\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c57ba49f",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "nldl-winter-school",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
